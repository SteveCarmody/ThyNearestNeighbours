---
title: "Machine Learning Competition"
author: "Stephen Carmody, Stefano Costantini, Monika Krzyzanowska"
date: "27 March 2015"
output: html_document
---
 
## Introduction

This technical report summarises the analysis that our team (__'Thy Nearest Neighbours'__) has carried out for the Machine Learning competition. The objective is to provide an overview of our approach, showing the techniques we have considered and the results that have led us make our final submission to the competition.

The report is structured as follows:

- First we describe the approach we have used to test the chosen techniques and to produce the results.
- Then we discuss our results relative to the application of the shortlisted techniques. For each technique we show the results of the testing that we used to choose the method underpinning our final submission.
- We conclude by showing our chosen technique and its results

## Initial assessment and approach

Our first step to address this classification problem was to identify a short-list of tecniques that could be used. We review the existing literature and identified a longlist of possible candidates. For each of these techniques we carried out some preliminary testing to check whether they warrated further assessement. 

During this initial testing phase, we decided to discard some approach that were not delivering acceptable results. For example, we tested the __Adaboost.M1__ algorithm (using the `R` package `adabag`) and found the error to be too high. Specifically, the test error on a 10% holdout sample was __0.282__ and the 10-fold cross validation error was __0.276.__ We decided not to pursue this approach further.

__[STEPHEN: COULD WE ADD SOMETHING ON NEURAL NETS]__

___Techniques shortlist___

At the end of this initial phase we identified a shortlist of techniques to explore further. These are:

- k-Nearest Neighbour rule
- Support Vector Machines __[NOT SURE WE SHOULD INCLUDE THIS, ERRORS ARE QUITE HIGH]__
- Random Forests
 
For each technique, we implemented the following testing procedure:

1. First we identified the set of parameters that we wished to change. On this basis we defined a "testing grid" to set out all possible combination of parameters.
2. We implemented the testing, parallelising the calculations over the "testing grid". We did this by means of the `foreach`, `snow` and `doSNOW` `R` packages. We trained each model on 40,000 observations and calculated the test error on a hold-out sample of 10,000 observation,  
3. We summarised the results of the multiple testing in a data table and selected the specification yielding the lowest test error.

___Feature construction___

__[ADD HERE A BRIEF DISCUSSION OF THE FEATURES WE HAVE CONSTRUCTED]__

### k-NN

We have tested a k-NN approach by considering these values of the parameter $k$, i.e. the number of neighbour observations to consider in order to assign a class:

- $k = 1,3,5,7,9,11,13,15,17,19,23,27,29,31,37,41,43,47,51$ 

The following table provides a summary of the 5 best results:

|# of trees| Test error |
|:--------:|:----------:|
| 1        |   0.0996   |
| 3        |   0.1181   |
| 5        |   0.1346   |
| 7        |   0.1431   |
| 9        |   0.1538   |

The code to reproduce these results is provided in our repository on GitHub in the file `kNN-ThyNearestNeighbours.R`.

On this basis, for this technique we selected a 1-NN approach.

### SVM

__[NOT SURE WE SHOULD INCLUDE THIS, IT DOES NOT LOOK TOO PROMISING]__

### Random Forest

We have tested a Random Forest approach by considering these ranges of parameters:

- __Number of trees in each iteration:__ from 50 to 1,450, with an increment of 50.
- __Number of features considered in each iteration:__ from 3 to 30, with an increment of 3.

The following table provides a summary of the 5 best results:

|# of trees| # of features | Test error |
|:--------:|:-------------:|:----------:|
| 1350     |  27           |   0.1082   |
| 450      |  30           |   0.1082   |
| 350      |  27           |   0.1083   |
| 950      |  30           |   0.1083   |
| 450      |  30           |   0.1084   |

The code to reproduce these results is provided in our repository on GitHub in the file `Random_Forest-ThyNearestNeighbours.R`.

On this basis, for this technique we selected a Random Forest approach with these parameters:

- Number of trees: 1350
- Number of features in each iteration: 27

## Conclusions

The following table summarises the best result that we have achieved with each of the techiques we have tested.

|Technique | Test Error |
|:---------|:----------:|
| kNN      |  0.0996    |
| SVM      |  0.0000    |
| RF       |  0.1082    |

On the basis of the results above, we have selected __[ADD SELECTED APPROACH]__, using the following parameters:

- Parameter 1
- Parameter 2
- Parameter 3

We have used this approach to make our final submission to Kaggle which yielded a score of __[REPORT KAGGLE SCORE OF OUR PREFERRED SUBMISSION]__





