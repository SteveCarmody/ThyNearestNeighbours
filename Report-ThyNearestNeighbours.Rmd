---
title: "Machine Learning Competition"
author: "Stephen Carmody, Stefano Costantini, Monika Krzyzanowska"
date: "27 March 2015"
output: html_document
---
 
## Introduction

This technical report summarises the analysis that our team (__'Thy Nearest Neighbours'__) has carried out for the Machine Learning competition. The objective is to provide an overview of our approach, showing the techniques we have considered and the results that have led us make our final submission to the competition.

The report is structured as follows:

- First we describe the approach we have used to test the chosen techniques and to produce the results.
- Then we discuss our results relative to the application of the shortlisted techniques. For each technique we show the results of the testing that we used to choose the method underpinning our final submission.
- We conclude by showing our chosen technique and its results

## Initial assessment and approach

Our first step to address this classification problem was to identify a short-list of tecniques that could be used. We review the existing literature and identified a longlist of possible candidates. For each of these techniques we carried out some preliminary testing to check whether they warrated further assessement. 

During this initial testing phase, we decided to discard some approach that were not delivering acceptable results. For example, we tested the __Adaboost.M1__ algorithm (using the `R` package `adabag`) and found the error to be too high. Specifically, the test error on a 10% holdout sample was __0.282__ and the 10-fold cross validation error was __0.276.__ We decided not to pursue this approach further.

__[STEPHEN: COULD WE ADD SOMETHING ON NEURAL NETS]__

At the end of this initial phase we identified a shortlist of techniques to explore further. These are:

- k-Nearest Neighbour rule
- Support Vector Machines __[NOT SURE WE SHOULD INCLUDE THIS, ERRORS ARE QUITE HIGH]__
- Bagging
- Random Forests
 
For each technique, we implemented the following testing procedure:

1. First we identified the set of parameters that we wished to change. On this basis we defined a "testing grid" to set out all possible combination of parameters.
2. We implemented the testing, parallelising the calculations over the "testing grid". We did this by means of the `foreach`, `snow` and `doSNOW` `R` packages. We trained each model on 40,000 observations and calculated the test error on a hold-out sample of 10,000 observation,  
3. We summarised the results of the multiple testing in a data table and selected the specification yielding the lowest test error.

### k-NN



### SVM

__[NOT SURE WE SHOULD INCLUDE THIS, IT DOES NOT LOOK TOO PROMISING]__

### Bagging



### Random Forest



##


