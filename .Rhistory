stump <- rpart(training.class~. , cbind(training, training.class) , w = w, control = c(maxdepth = 1))
class <- predict(stump, training , training.class, type="class")
class <- predict(stump, cbind(training, training.class), type="class")
e <- mean(training.class!=class)
e
sampleIndex <- sample(n,size = (n*.25))
test <- data[sampleIndex,1:(ncol(data))]
training <- data[-sampleIndex,1:(ncol(data) )]
stump <- rpart(Y~.,training, w = w, control = c(maxdepth = 1))
class <- predict(stump, training, type="class")
e <- mean(training$Y!=class)
e
sampleIndex <- sample(n,size = (n*.25))
test <- data[sampleIndex,1:(ncol(data) - 1)]
test.class <- data[sampleIndex,ncol(data)]
training <- data[-sampleIndex,1:(ncol(data) - 1)]
training.class <- data[sampleIndex,ncol(data)]
stump <- rpart(training.class~. , cbind(training, training.class) , w = w, control = c(maxdepth = 1))
class <- predict(stump, cbind(training, training.class), type="class")
class
predict
?predict
class <- predict(stump, cbind(training, Y=  training.class), type="class")
class
sprintf("$ %3.2f", 1)
sprintf("I %3.2f", 1)
sprintf("I %3.2f"\t, 1)
sprintf("I %3.2f" \t, 1)
sprintf("I %3.2f" /t, 1)
sprintf("I %3.2f"  "Error", 1)
sprintf("I %3.2f" , "Error", 1)
sprintf("I %3.2f  Error %i", 1, 2)
sprintf("I %3.2f  Error %i  Aplha %a", 1, 2, 3)
sprintf("I %3.2f  Error %i  Aplha %i", 1, 2, 3)
sprintf("I %d  Error %i  Aplha %i", 1, 2, 3)
sprintf("I %d  Error %i  Aplha %i", e, 2, 3)
i <- 1
print(i)
# Create a classification tree with depth 1 (rpart allows for NA values)
# & predict stump classes and the prediction error
stump <- rpart(Y~.,training, w = w, control = c(maxdepth = 1))
class <- predict(stump, training, type="class")
sampleIndex <- sample(n,size = (n*.25))
test <- data[sampleIndex,1:(ncol(data))]
training <- data[-sampleIndex,1:(ncol(data) )]
i <- 1
print(i)
stump <- rpart(Y~.,training, w = w, control = c(maxdepth = 1))
class <- predict(stump, training, type="class")
e <- mean(training$Y!=class)
print(i)
cat("Training error:")
cat(e)
cat("|")
alpha <- (1/2) * log( (1-e)/(e) )
cat("alpha:")
cat(alpha)
cat("\n")
sprintf("I %d  Error %i  Aplha %i", e, 2, 3)
sprintf("I %i  Error %i  Aplha %i", e, 2, 3)
sprintf("I %1.2f  Error %i  Aplha %i", e, 2, 3)
sprintf("I %1.5f  Error %i  Aplha %i", e, 2, 3)
sprintf("Iteration %i  Error %1.5f  Aplha %i", i, e, 3)
sprintf("Iteration %i,  Error %1.5f  Aplha %i", i, e, 3)
sprintf("Iteration %i,  Error =  %1.5f &  Aplha = %1.5f", i, e, alpha)
sprintf("For iteration %i,  error =  %1.5f  &  Aplha = %1.5f", i, e, alpha)
sprintf("For iteration %i,  error =  %1.5f  -  Aplha = %1.5f", i, e, alpha)
sprintf("For iteration %i,  error =  %1.5f  and  Aplha = %1.5f", i, e, alpha)
#########################################################################################################
#                                                                                                       #
#   Author: Stephen Carmody                                                                             #
#   Title:  Machine Learning 6                                                                          #
#   Ref:    http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf      #
#                                                                                                       #
#########################################################################################################
library(rpart)
library(mvtnorm)
library(mlbench)
library(randomForest)
library(ggplot2)
# Create data for classificatiob
n <- 5000
Y.one <- rmvnorm(n, mean = c(4,5.5), matrix(c(1,0.15,0.15,1),2))
Y.two <- rmvnorm(n, mean = c(6.5,4.5), matrix(c(1,0.35,0.35,1),2))
data <- as.data.frame(rbind(Y.one,Y.two))
data <- cbind(data,c(rep(1,length=n),rep(-1,length=n)))
colnames(data) <- c("Var1","Var2","Y")
# Convert the class to a factor for classification
data[,ncol(data)] <- as.factor(data[,ncol(data)])
# Split the data
sampleIndex <- sample(n,size = (n*.25))
data.test <- data[sampleIndex,1:(ncol(data))]
data.train <- data[-sampleIndex,1:(ncol(data) )]
#  Adaboost function
adaboost.M1 <- function(data.train,data.test,max.iter=10) {
#initialize weights, alpha and preiction matrix
w <- rep(1/nrow(data.train),nrow(data.train))
alpha <- 1
pred <- matrix(NA,nrow(data.test),max.iter)
# weak learner predictions
for (i in 1:max.iter) {
# Create a classification tree with depth 1 (rpart allows for NA values)
# & predict stump classes and the prediction error
stump <- rpart(Y~.,data.train, w = w, control = c(maxdepth = 1))
class <- predict(stump, data.train, type="class")
e <- mean(data.train$Y!=class)
# calculate alpha
alpha <- (1/2) * log( (1-e)/(e) )
# Caculate performace
perf <- data.train$Y==class
perf[data.train$Y==class] <- -1
perf[data.train$Y!=class] <- 1
w <- w * exp(0.1 * perf * alpha)
pred[,i] <- as.numeric(as.vector(predict(stump,test,type="class"))) * alpha
sprintf("For iteration %i,  error =  %1.5f  and  Aplha = %1.5f", i, e, alpha)
}
# calculate final classification
classes.pred <- sign(rowSums(pred, na.rm = TRUE))
return(classes.pred)
}
# Get test error as number of trees vary
numTrees <- seq(1,50)
test.error <- rep(0,length(numTrees))
for (i in numTrees) {
prediction <- adaboost.M1(training,test,i)
test.error[i] <- mean(prediction!=test$Y)
}
# Bagging
bagging.test.error <- rep(0,length(numTrees))
for (i in numTrees) {
bagging <- randomForest(Y ~ ., training, mtry=(ncol(data)-1), ntree=i)
bagging.prediction <- predict(bagging, test, type="class")
bagging.test.error[i] <- mean(bagging.prediction!=test$Y)
}
# randomForest
rf.test.error <- rep(0,length(numTrees))
for (i in numTrees) {
rf.model <- randomForest(Y ~ ., training, ntree=i)
rf.prediction <- predict(rf.model, test, type="class")
rf.test.error[i] <- mean(rf.prediction!=test$Y)
}
plot(numTrees,test.error,type="l",col="black", ylim=c(0.05,0.2), ylab="Test Error", xlab="Number of Trees",main="Comparison of Methods")
lines(numTrees,bagging.test.error, type="l", col="red")
lines(numTrees,rf.test.error, type="l", col="green")
legend(0,0.2,c("AdaBoost","Bagging","Random Forest"), cex=1, lty=c(1,1,1), bty="n", col=c("black","green","red"), ncol=1)
#########################################################################################################
#                                                                                                       #
#   Author: Stephen Carmody                                                                             #
#   Title:  Machine Learning 6                                                                          #
#   Ref:    http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf      #
#                                                                                                       #
#########################################################################################################
library(rpart)
library(mvtnorm)
library(mlbench)
library(randomForest)
library(ggplot2)
# Create data for classificatiob
n <- 5000
Y.one <- rmvnorm(n, mean = c(4,5.5), matrix(c(1,0.15,0.15,1),2))
Y.two <- rmvnorm(n, mean = c(6.5,4.5), matrix(c(1,0.35,0.35,1),2))
data <- as.data.frame(rbind(Y.one,Y.two))
data <- cbind(data,c(rep(1,length=n),rep(-1,length=n)))
colnames(data) <- c("Var1","Var2","Y")
# Convert the class to a factor for classification
data[,ncol(data)] <- as.factor(data[,ncol(data)])
# Split the data
sampleIndex <- sample(n,size = (n*.25))
data.test <- data[sampleIndex,1:(ncol(data))]
data.train <- data[-sampleIndex,1:(ncol(data) )]
#  Adaboost function
adaboost.M1 <- function(data.train,data.test,max.iter=10) {
#initialize weights, alpha and preiction matrix
w <- rep(1/nrow(data.train),nrow(data.train))
alpha <- 1
pred <- matrix(NA,nrow(data.test),max.iter)
# weak learner predictions
for (i in 1:max.iter) {
# Create a classification tree with depth 1 (rpart allows for NA values)
# & predict stump classes and the prediction error
stump <- rpart(Y~.,data.train, w = w, control = c(maxdepth = 1))
class <- predict(stump, data.train, type="class")
e <- mean(data.train$Y!=class)
# calculate alpha
alpha <- (1/2) * log( (1-e)/(e) )
# Caculate performace
perf <- data.train$Y==class
perf[data.train$Y==class] <- -1
perf[data.train$Y!=class] <- 1
w <- w * exp(0.1 * perf * alpha)
pred[,i] <- as.numeric(as.vector(predict(stump,data.test,type="class"))) * alpha
sprintf("For iteration %i,  error =  %1.5f  and  Aplha = %1.5f", i, e, alpha)
}
# calculate final classification
classes.pred <- sign(rowSums(pred, na.rm = TRUE))
return(classes.pred)
}
test
data.test
# Get test error as number of trees vary
numTrees <- seq(1,50)
test.error <- rep(0,length(numTrees))
for (i in numTrees) {
prediction <- adaboost.M1(data.train,data.test,i)
test.error[i] <- mean(prediction!=data.test$Y)
}
#########################################################################################################
#                                                                                                       #
#   Author: Stephen Carmody                                                                             #
#   Title:  Machine Learning 6                                                                          #
#   Ref:    http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf      #
#                                                                                                       #
#########################################################################################################
library(rpart)
library(mvtnorm)
library(mlbench)
library(randomForest)
library(ggplot2)
# Create data for classificatiob
n <- 5000
Y.one <- rmvnorm(n, mean = c(4,5.5), matrix(c(1,0.15,0.15,1),2))
Y.two <- rmvnorm(n, mean = c(6.5,4.5), matrix(c(1,0.35,0.35,1),2))
data <- as.data.frame(rbind(Y.one,Y.two))
data <- cbind(data,c(rep(1,length=n),rep(-1,length=n)))
colnames(data) <- c("Var1","Var2","Y")
# Convert the class to a factor for classification
data[,ncol(data)] <- as.factor(data[,ncol(data)])
# Split the data
sampleIndex <- sample(n,size = (n*.25))
data.test <- data[sampleIndex,1:(ncol(data))]
data.train <- data[-sampleIndex,1:(ncol(data) )]
#  Adaboost function
adaboost.M1 <- function(data.train,data.test,max.iter=10) {
#initialize weights, alpha and preiction matrix
w <- rep(1/nrow(data.train),nrow(data.train))
alpha <- 1
pred <- matrix(NA,nrow(data.test),max.iter)
# weak learner predictions
for (i in 1:max.iter) {
# Create a classification tree with depth 1 (rpart allows for NA values)
# & predict stump classes and the prediction error
stump <- rpart(Y~.,data.train, w = w, control = c(maxdepth = 1))
class <- predict(stump, data.train, type="class")
e <- mean(data.train$Y!=class)
# calculate alpha
alpha <- (1/2) * log( (1-e)/(e) )
# Caculate performace
perf <- data.train$Y==class
perf[data.train$Y==class] <- -1
perf[data.train$Y!=class] <- 1
w <- w * exp(0.1 * perf * alpha)
pred[,i] <- as.numeric(as.vector(predict(stump,data.test,type="class"))) * alpha
sprintf("For iteration %i,  error =  %1.5f  and  Aplha = %1.5f", i, e, alpha)
}
# calculate final classification
classes.pred <- sign(rowSums(pred, na.rm = TRUE))
return(classes.pred)
}
# Get test error as number of trees vary
numTrees <- seq(1,50)
test.error <- rep(0,length(numTrees))
for (i in numTrees) {
prediction <- adaboost.M1(data.train,data.test,i)
test.error[i] <- mean(prediction!=data.test$Y)
}
# Bagging
bagging.test.error <- rep(0,length(numTrees))
for (i in numTrees) {
bagging <- randomForest(Y ~ ., data.train, mtry=(ncol(data)-1), ntree=i)
bagging.prediction <- predict(bagging, data.test, type="class")
bagging.test.error[i] <- mean(bagging.prediction!=data.test$Y)
}
# randomForest
rf.test.error <- rep(0,length(numTrees))
for (i in numTrees) {
rf.model <- randomForest(Y ~ ., data.train, ntree=i)
rf.prediction <- predict(rf.model, data.test, type="class")
rf.test.error[i] <- mean(rf.prediction!=data.test$Y)
}
plot(numTrees,test.error,type="l",col="black", ylim=c(0.05,0.2), ylab="Test Error", xlab="Number of Trees",main="Comparison of Methods")
lines(numTrees,bagging.test.error, type="l", col="red")
lines(numTrees,rf.test.error, type="l", col="green")
legend(0,0.2,c("AdaBoost","Bagging","Random Forest"), cex=1, lty=c(1,1,1), bty="n", col=c("black","green","red"), ncol=1)
sprintf("For iteration %i,  error =  %1.5f  and  Aplha = %1.5f", i, e, alpha)
sampleIndex <- sample(n,size = (n*.25))
data.test <- data[sampleIndex,1:(ncol(data))]
data.train <- data[-sampleIndex,1:(ncol(data) )]
w <- rep(1/nrow(data.train),nrow(data.train))
alpha <- 1
pred <- matrix(NA,nrow(data.test),max.iter)
max.iter <- 10
i <- 1
# Create a classification tree with depth 1 (rpart allows for NA values)
# & predict stump classes and the prediction error
stump <- rpart(Y~.,data.train, w = w, control = c(maxdepth = 1))
class <- predict(stump, data.train, type="class")
e <- mean(data.train$Y!=class)
# calculate alpha
alpha <- (1/2) * log( (1-e)/(e) )
# Caculate performace
perf <- data.train$Y==class
perf[data.train$Y==class] <- -1
perf[data.train$Y!=class] <- 1
w <- w * exp(0.1 * perf * alpha)
pred[,i] <- as.numeric(as.vector(predict(stump,data.test,type="class"))) * alpha
sprintf("For iteration %i,  error =  %1.5f  and  Aplha = %1.5f", i, e, alpha)
max.iter <- 10
#initialize weights, alpha and preiction matrix
w <- rep(1/nrow(data.train),nrow(data.train))
alpha <- 1
pred <- matrix(NA,nrow(data.test),max.iter)
i <- 1
# Create a classification tree with depth 1 (rpart allows for NA values)
# & predict stump classes and the prediction error
stump <- rpart(Y~.,data.train, w = w, control = c(maxdepth = 1))
class <- predict(stump, data.train, type="class")
e <- mean(data.train$Y!=class)
# calculate alpha
alpha <- (1/2) * log( (1-e)/(e) )
# Caculate performace
perf <- data.train$Y==class
perf[data.train$Y==class] <- -1
perf[data.train$Y!=class] <- 1
w <- w * exp(0.1 * perf * alpha)
pred[,i] <- as.numeric(as.vector(predict(stump,data.test,type="class"))) * alpha
sprintf("For iteration %i,  error =  %1.5f  and  Aplha = %1.5f", i, e, alpha)
print(sprintf("For iteration %i,  error =  %1.5f  and  Aplha = %1.5f", i, e, alpha))
#########################################################################################################
#                                                                                                       #
#   Author: Stephen Carmody                                                                             #
#   Title:  Machine Learning 6                                                                          #
#   Ref:    http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf      #
#                                                                                                       #
#########################################################################################################
library(rpart)
library(mvtnorm)
library(mlbench)
library(randomForest)
library(ggplot2)
# Create data for classificatiob
n <- 5000
Y.one <- rmvnorm(n, mean = c(4,5.5), matrix(c(1,0.15,0.15,1),2))
Y.two <- rmvnorm(n, mean = c(6.5,4.5), matrix(c(1,0.35,0.35,1),2))
data <- as.data.frame(rbind(Y.one,Y.two))
data <- cbind(data,c(rep(1,length=n),rep(-1,length=n)))
colnames(data) <- c("Var1","Var2","Y")
# Convert the class to a factor for classification
data[,ncol(data)] <- as.factor(data[,ncol(data)])
# Split the data
sampleIndex <- sample(n,size = (n*.25))
data.test <- data[sampleIndex,1:(ncol(data))]
data.train <- data[-sampleIndex,1:(ncol(data) )]
#  Adaboost function
adaboost.M1 <- function(data.train,data.test,max.iter=10) {
#initialize weights, alpha and preiction matrix
w <- rep(1/nrow(data.train),nrow(data.train))
alpha <- 1
pred <- matrix(NA,nrow(data.test),max.iter)
# weak learner predictions
for (i in 1:max.iter) {
# Create a classification tree with depth 1 (rpart allows for NA values)
# & predict stump classes and the prediction error
stump <- rpart(Y~.,data.train, w = w, control = c(maxdepth = 1))
class <- predict(stump, data.train, type="class")
e <- mean(data.train$Y!=class)
# calculate alpha
alpha <- (1/2) * log( (1-e)/(e) )
# Caculate performace
perf <- data.train$Y==class
perf[data.train$Y==class] <- -1
perf[data.train$Y!=class] <- 1
w <- w * exp(0.1 * perf * alpha)
pred[,i] <- as.numeric(as.vector(predict(stump,data.test,type="class"))) * alpha
print(sprintf("For iteration %i,  error =  %1.5f  and  Aplha = %1.5f", i, e, alpha))
}
# calculate final classification
classes.pred <- sign(rowSums(pred, na.rm = TRUE))
return(classes.pred)
}
# Get test error as number of trees vary
numTrees <- seq(1,50)
test.error <- rep(0,length(numTrees))
for (i in numTrees) {
prediction <- adaboost.M1(data.train,data.test,i)
test.error[i] <- mean(prediction!=data.test$Y)
}
# Bagging
bagging.test.error <- rep(0,length(numTrees))
for (i in numTrees) {
bagging <- randomForest(Y ~ ., data.train, mtry=(ncol(data)-1), ntree=i)
bagging.prediction <- predict(bagging, data.test, type="class")
bagging.test.error[i] <- mean(bagging.prediction!=data.test$Y)
}
# randomForest
rf.test.error <- rep(0,length(numTrees))
for (i in numTrees) {
rf.model <- randomForest(Y ~ ., data.train, ntree=i)
rf.prediction <- predict(rf.model, data.test, type="class")
rf.test.error[i] <- mean(rf.prediction!=data.test$Y)
}
plot(numTrees,test.error,type="l",col="black", ylim=c(0.05,0.2), ylab="Test Error", xlab="Number of Trees",main="Comparison of Methods")
lines(numTrees,bagging.test.error, type="l", col="red")
lines(numTrees,rf.test.error, type="l", col="green")
legend(0,0.2,c("AdaBoost","Bagging","Random Forest"), cex=1, lty=c(1,1,1), bty="n", col=c("black","green","red"), ncol=1)
ggplot(aes(x = numTrees, y =test.error))
class(test.error)
class(numTrees)
ggplot()
+geom_line(numTrees,bagging.test.error, type="l", col="red")
?geom_line
+geom_line(data = (numTrees,bagging.test.error), type="l", col="red")
test.error
numTrees
results <- data.frame(Trees = numTrees, Error = test.error)
ggplot(results)
+geom_line(x = results$Trees,results$Error, col="red")
+geom_line(aes(x = results$Trees,results$Error, col="red"))
ggplot(results) + geom_line(aes(x = results$Trees,results$Error, col="red"))
results <- data.frame(Trees = numTrees,
Adaboost.error = test.error,
RandomForest.Error = rf.test.error,
Bagging.Error = bagging.test.error)
plot(numTrees,test.error,type="l",col="black", ylim=c(0.05,0.2), ylab="Test Error", xlab="Number of Trees",main="Comparison of Methods")
ggplot(results) + geom_line(aes(x = results$Trees,results$Error, col="red"))
ggplot(results) + geom_line(aes(x = results$Trees,results$Error, col="red"))
results <- data.frame(Trees = numTrees,
Adaboost.error = test.error,
RandomForest.Error = rf.test.error,
Bagging.Error = bagging.test.error)
ggplot(results) + geom_line(aes(x = results$Trees,results$Error, col="red"))
ggplot(results) + geom_line(aes(x = results$Trees,results$Adaboost.error, col="red"))
ggplot(results)
+ geom_line(aes(x = results$Trees,results$Adaboost.error, col="red"))
ggplot(results) +
geom_line(aes(x = results$Trees,results$Adaboost.error, col="red"))
ggplot(results) +
geom_line(aes(x = results$Trees,results$Adaboost.error, col="black")) +
geom_line(aes(x = results$Trees,results$RandomForest.Error, col="green")) +
geom_line(aes(x = results$Trees,results$Bagging.Error, col="red")) +
ggplot(results) +
geom_line(aes(x = results$Trees,results$Adaboost.error, col="black")) +
geom_line(aes(x = results$Trees,results$RandomForest.Error, col="green")) +
geom_line(aes(x = results$Trees,results$Bagging.Error, col="red"))
ggplot(results) +
geom_line(aes(x = results$Trees,results$Adaboost.error, col="black")) +
geom_line(aes(x = results$Trees,results$RandomForest.Error, col="green")) +
geom_line(aes(x = results$Trees,results$Bagging.Error, col="red"))
ggplot(results) +
geom_line(aes(x = results$Trees,results$Adaboost.error, col="black")) +
geom_line(aes(x = results$Trees,results$RandomForest.Error, col="green")) +
geom_line(aes(x = results$Trees,results$Bagging.Error, col="red")) +
ggtitle("Comparison of Methods")
ggplot(results) +
geom_line(aes(x = results$Trees,results$Adaboost.error, col="black")) +
geom_line(aes(x = results$Trees,results$RandomForest.Error, col="green")) +
geom_line(aes(x = results$Trees,results$Bagging.Error, col="red")) +
xlab("Number of Trees") +
ggtitle("Comparison of Methods")
ggplot(results) +
geom_line(aes(x = results$Trees,results$Adaboost.error, col="black")) +
geom_line(aes(x = results$Trees,results$RandomForest.Error, col="green")) +
geom_line(aes(x = results$Trees,results$Bagging.Error, col="red")) +
xlab("Number of Trees") + ylab("Test Error")
ggtitle("Comparison of Methods")
ggplot(results) +
geom_line(aes(x = results$Trees,results$Adaboost.error, col="black")) +
geom_line(aes(x = results$Trees,results$RandomForest.Error, col="green")) +
geom_line(aes(x = results$Trees,results$Bagging.Error, col="red")) +
xlab("Number of Trees") + ylab("Test Error") + theme_bw() +
scale_fill_manual(name="Mthod", labels=c("Adaboost M1", "Random Forest", "Bagging")) +
ggtitle("Comparison of Methods")
scale_fill_manual(values=c("#black", "#green", "#red"), name="Method", labels=c("Adaboost M1", "Random Forest", "Bagging")) +
ggtitle("Comparison of Methods")
ggplot(results) +
geom_line(aes(x = results$Trees,results$Adaboost.error, col="black")) +
geom_line(aes(x = results$Trees,results$RandomForest.Error, col="green")) +
geom_line(aes(x = results$Trees,results$Bagging.Error, col="red")) +
xlab("Number of Trees") + ylab("Test Error") + theme_bw() +
scale_fill_manual(values=c("#black", "#green", "#red"), name="Method", labels=c("Adaboost M1", "Random Forest", "Bagging")) +
ggtitle("Comparison of Methods")
ggplot(results) +
geom_line(aes(x = results$Trees,results$Adaboost.error, col="black")) +
geom_line(aes(x = results$Trees,results$RandomForest.Error, col="green")) +
geom_line(aes(x = results$Trees,results$Bagging.Error, col="red")) +
xlab("Number of Trees") + ylab("Test Error") + theme_bw() +
scale_fill_manual(values=c("Adaboost", "#green", "#red"), name="Method", labels=c("Adaboost M1", "Random Forest", "Bagging")) +
ggtitle("Comparison of Methods")
ggplot(results) +
geom_line(aes(x = results$Trees,results$Adaboost.error, col="black")) +
geom_line(aes(x = results$Trees,results$RandomForest.Error, col="green")) +
geom_line(aes(x = results$Trees,results$Bagging.Error, col="red")) +
xlab("Number of Trees") + ylab("Test Error") + theme_bw() +
scale_fill_discrete(name="Method", labels=c("Adaboost M1", "Random Forest", "Bagging")) +
ggtitle("Comparison of Methods")
ggplot(results) +
geom_line(aes(x = results$Trees,results$Adaboost.error, col="black")) +
geom_line(aes(x = results$Trees,results$RandomForest.Error, col="green")) +
geom_line(aes(x = results$Trees,results$Bagging.Error, col="red")) +
xlab("Number of Trees") + ylab("Test Error") + theme_bw() +
scale_fill_discrete(name="Experimental\nCondition",
breaks=c("ctrl", "trt1", "trt2"),
labels=c("Control", "Treatment 1", "Treatment 2")) +
ggtitle("Comparison of Methods")
ggplot(results) +
geom_line(aes(x = results$Trees,results$Adaboost.error, col="black")) +
geom_line(aes(x = results$Trees,results$RandomForest.Error, col="green")) +
geom_line(aes(x = results$Trees,results$Bagging.Error, col="red")) +
xlab("Number of Trees") + ylab("Test Error") + theme_bw() +
scale_fill_discrete(name="Experimental\nCondition",
breaks=c("ctrl", "trt1", "trt2"),
labels=c("Control", "Treatment 1", "Treatment 2")) +
ggtitle("Comparison of Methods")
